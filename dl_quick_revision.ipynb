{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Neural Network (NN)**"
      ],
      "metadata": {
        "id": "ixPCFnh7Mm9L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Neural Network (NN)**\n",
        "- **ANN** (Artificial Neural Network)\n",
        "  - Feedforward / MLP\n",
        "- **CNN** (Convolutional Neural Network)\n",
        "- **RNN** (Recurrent Neural Network) → LSTM, GRU\n",
        "- **Transformer** → BERT, GPT\n",
        "- **Autoencoder** → Variational Autoencoder (VAE)\n",
        "- **GAN** (Generative Adversarial Network)"
      ],
      "metadata": {
        "id": "Vq9RLZ9IMzT5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **ANN →  Artificial Neural Network**"
      ],
      "metadata": {
        "id": "H9cQNBY5YGpI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **FNN →   Feedforward Neural Network**"
      ],
      "metadata": {
        "id": "UP5pDtD2YIGC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Each neuron take input- input will be multiplied by weight- calculate weighted sum of each layer -  add bias\n",
        "\n",
        "    **`∑ ( 𝑤𝑖 ⋅ 𝑥𝑖 ) + 𝑏`**\n",
        "\n",
        "- Apply activation function - get the output\n",
        "\n",
        "    **`𝑦 = Activation ( ∑ ( 𝑤 𝑖 ⋅ 𝑥 𝑖 ) + 𝑏 )`**\n",
        "\n",
        "- Result (y) will pass to the next layer until the output layer gives a final prediction.\n",
        "\n",
        "- Calculate loss: Compare prediction with actual value\n",
        "\n",
        "- Improve Accuracy using backpropagation: Errors are sent backward through the layers - Adjust weight to reduce error\n",
        "\n",
        "    **`𝑤 = 𝑤 − 𝛼 ⋅ ∂ Loss / ∂ 𝑤`**\n",
        "\n",
        "- Repeat Until Convergence - Train multiple **epochs** until the model **makes accurate predictions**.\n"
      ],
      "metadata": {
        "id": "qXGvIdpuYIIW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **CNN → Convolutional Neural Networks**"
      ],
      "metadata": {
        "id": "qH4jxJQ4T6Ay"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Start with **input image (or data)** – break it into small patches.\n",
        "\n",
        "- Apply **filters (kernels)** – slide over input – perform element-wise multiplication – calculate features (edges, textures, etc.).\n",
        "\n",
        "     **`Feature Map = ∑ (Filter × Image Patch) + Bias`**\n",
        "\n",
        "- Apply activation function (usually ReLU) – keep only important features.\n",
        "\n",
        "    **`𝑦 = Activation ( ∑ (Filter × Patch) + 𝑏 )`**\n",
        "\n",
        "- Apply **Pooling (e.g., Max Pooling)** – reduces size – keeps strongest features.\n",
        "\n",
        "    **`Pooling = Pick maximum or average value from patch`**\n",
        "\n",
        "- Repeat: Convolution → Activation → Pooling to learn deeper patterns.\n",
        "\n",
        "  -Flatten the final feature map – turn it into a vector.\n",
        "\n",
        "- Pass to **Fully Connected Layers (like ANN)** to make predictions.\n",
        "\n",
        "- Calculate Loss: Compare predicted output with actual value.\n",
        "\n",
        "- Improve Accuracy using **Backpropagation**: Send error backward – update filter and dense layer weights.\n",
        "\n",
        "    **`𝑤 = 𝑤 − 𝛼 ⋅ ∂ Loss / ∂ 𝑤`**\n",
        "\n",
        "- Repeat Until Convergence – train for multiple epochs until model predicts correctly."
      ],
      "metadata": {
        "id": "1-liSZCaS3Dq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **RNN → Recurrent Neural Networks**"
      ],
      "metadata": {
        "id": "X2I2pSIsV1Y6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Start with **input data (like sequence/text/time series)** – pass one value at a time (step-by-step).\n",
        "\n",
        "- Each step: Multiply input by weight, add previous hidden state (memory), apply activation.\n",
        "\n",
        "    **`ℎₜ = Activation ( 𝑤ₓ ⋅ 𝑥ₜ + 𝑤ₕ ⋅ ℎₜ₋₁ + 𝑏 )`**\n",
        "\n",
        "- Pass the new hidden state ℎₜ to the next step – this helps the network \"remember\" past info.\n",
        "\n",
        "- Final output is compared with actual value – calculate loss.\n",
        "\n",
        "- Backpropagation Through Time (BPTT): Send error backward through all time steps – update weights.\n",
        "\n",
        "    **`𝑤 = 𝑤 − 𝛼 ⋅ ∂ Loss / ∂ 𝑤`**\n",
        "\n",
        "- Repeat until convergence – train over sequences multiple times.\n"
      ],
      "metadata": {
        "id": "QV6jC11ATZ8q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**LSTM (Long Short-Term Memory)**\n",
        "\n",
        "Same as RNN but with **extra gates** to **better handle long-term memory**.\n",
        "\n",
        "- **Forget Gate**: Decides what to throw away from memory.\n",
        "- **Input Gate**: Decides what new info to add.\n",
        "- **Output Gate**: Decides what to show as output.\n",
        "\n",
        "All gates work together to update the cell state (memory) and hidden state.\n",
        "\n",
        "This solves the problem of **vanishing gradient** in long sequences.\n",
        "\n",
        "|\n",
        "\n",
        "**GRU (Gated Recurrent Unit)**\n",
        "\n",
        "Similar to LSTM but **simpler** – uses only **2 gates**:\n",
        "\n",
        "- **Update Gate**: Controls both memory and input.\n",
        "- **Reset Gate**: Controls how much past info to forget.\n",
        "\n",
        "Faster and lighter than LSTM, often with similar performance."
      ],
      "metadata": {
        "id": "TmnfAboFWX3I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Transformer**"
      ],
      "metadata": {
        "id": "iCYx4FTHFKi7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Take **input sequence** (e.g., sentence) and convert to **embedding vectors**\n",
        "- Add **Positional Encoding** → adds info about word order\n",
        "- Pass through **Self-Attention** layers:\n",
        "  - Each word looks at other words and decides what to focus on\n",
        "  - Calculates weighted importance = **\"Attention\"**\n",
        "- Each layer follows:\n",
        "  - Self-Attention → Add & Normalize\n",
        "  - Feedforward Layer → Add & Normalize\n",
        "- Output is passed to the next layer or final prediction layer\n",
        "- Use **Loss Function** to compare prediction with actual\n",
        "- Use **Backpropagation** to update weights and learn\n",
        "- Repeat over many epochs"
      ],
      "metadata": {
        "id": "bGIPrftLJAcK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**BERT (Bidirectional Encoder Representations from Transformers)**\n",
        "\n",
        "- Uses only the **Encoder** part of Transformer\n",
        "- Reads sentence **in both directions (left + right)** for full context\n",
        "- Pretrained using **Masked Language Modeling** (hide random words and guess them)\n",
        "- Good for **understanding tasks**: classification, question answering, etc.\n",
        "\n",
        "|\n",
        "\n",
        "**GPT (Generative Pretrained Transformer)**\n",
        "\n",
        "- Uses only the **Decoder** part of Transformer\n",
        "- Reads text **left to right (one direction only)**\n",
        "- Pretrained to **predict the next word** in sequence\n",
        "- Great for **generating text**: writing, answering, summarizing, etc."
      ],
      "metadata": {
        "id": "uW59G7BjKuMT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Autoencoder**"
      ],
      "metadata": {
        "id": "9hyjg4FmLcw7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Input data (e.g., image or vector) is passed to the model.\n",
        "- **Encoder** compresses the input into a smaller representation (**latent vector**).\n",
        "- **Latent Vector** = compressed version of important features.\n",
        "- **Decoder** tries to **reconstruct the original input** from the latent vector.\n",
        "- Output is compared with original input → calculate **Reconstruction Loss**.\n",
        "- Backpropagation is used to adjust weights and **minimize reconstruction error**.\n",
        "- Trained to **learn only the essential features** of input data."
      ],
      "metadata": {
        "id": "Ts0gslv1Nxl6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Variational Autoencoder (VAE)**\n",
        "\n",
        "- Works like autoencoder but **adds randomness** to latent space.\n",
        "- Encoder doesn't give just one latent vector → it gives **mean (μ)** and **variance (σ)**.\n",
        "- From μ and σ → sample a **random point (z)** in latent space.\n",
        "- This makes the model **generate new, similar data** (good for generative tasks).\n",
        "- Loss = Reconstruction Loss + KL Divergence (regularizes latent space)\n",
        "- Trained to **generate new data** and learn **smooth, continuous features**."
      ],
      "metadata": {
        "id": "XSRi-RHaOBza"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Generative Adversarial Network (GAN)**"
      ],
      "metadata": {
        "id": "CBAVHtNFQQRd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- GAN has **two models**:  \n",
        "  - **Generator**: creates fake data  \n",
        "  - **Discriminator**: checks if data is real or fake  \n",
        "- Start with random noise as input → **Generator** tries to create realistic data (e.g., fake images).\n",
        "- **Discriminator** sees both real and fake data → predicts if it's real (1) or fake (0).\n",
        "- Both models compete:\n",
        "  - Generator improves to **fool the discriminator**\n",
        "  - Discriminator improves to **catch fakes**\n",
        "- Loss:\n",
        "  - Generator tries to **maximize** the chance of fooling the discriminator\n",
        "  - Discriminator tries to **minimize** its error in detecting fake vs real\n",
        "- Use **backpropagation** to update both models\n",
        "- Trained until **Generator produces realistic data** that Discriminator can’t easily detect"
      ],
      "metadata": {
        "id": "WopFws3WPZXS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Deep Reinforcement Learning (DRL)**"
      ],
      "metadata": {
        "id": "gB_VAVBkS7Ur"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- An **agent** interacts with an **environment** (like a game or simulation).\n",
        "- At each step:\n",
        "  - Agent observes the **state** of the environment.\n",
        "  - Chooses an **action** based on current state.\n",
        "  - Environment returns a **reward** and new **state**.\n",
        "- Goal: **Learn actions** that maximize total reward over time.\n",
        "- Uses **Deep Neural Networks** to approximate the best actions (called a policy or Q-function).\n",
        "- Learns by **trial and error**: good actions get **positive rewards**, bad ones get **penalties**.\n",
        "- Uses **backpropagation** to update the neural network and improve decisions.\n",
        "- Trained over many episodes until the agent learns the best strategy."
      ],
      "metadata": {
        "id": "UnZdXdJZS0Rb"
      }
    }
  ]
}