{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Neural Network (NN)**"
      ],
      "metadata": {
        "id": "ixPCFnh7Mm9L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Neural Network (NN)**\n",
        "- **ANN** (Artificial Neural Network) → Feedforward / MLP\n",
        "- **CNN** (Convolutional Neural Network)\n",
        "- **RNN** (Recurrent Neural Network) → LSTM, GRU\n",
        "- **Transformer** → BERT, GPT\n",
        "- **Autoencoder** → Variational Autoencoder (VAE)\n",
        "- **GAN** (Generative Adversarial Network)"
      ],
      "metadata": {
        "id": "Vq9RLZ9IMzT5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **ANN →  Artificial Neural Network**"
      ],
      "metadata": {
        "id": "H9cQNBY5YGpI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**FNN →  Feedforward Neural Network**\n",
        "\n",
        "- Each neuron take input- input will be multiplied by weight- calculate weighted sum of each layer -  add bias\n",
        "\n",
        "    **`∑ ( 𝑤𝑖 ⋅ 𝑥𝑖 ) + 𝑏`**\n",
        "\n",
        "- Apply activation function - get the output\n",
        "\n",
        "    **`𝑦 = Activation ( ∑ ( 𝑤 𝑖 ⋅ 𝑥 𝑖 ) + 𝑏 )`**\n",
        "\n",
        "- Result (y) will pass to the next layer until the output layer gives a final prediction.\n",
        "\n",
        "- Calculate loss: Compare prediction with actual value\n",
        "\n",
        "- Improve Accuracy using backpropagation: Errors are sent backward through the layers - Adjust weight to reduce error\n",
        "\n",
        "    **`𝑤 = 𝑤 − 𝛼 ⋅ ∂ Loss / ∂ 𝑤`**\n",
        "\n",
        "- Repeat Until Convergence - Train multiple **epochs** until the model **makes accurate predictions**.\n"
      ],
      "metadata": {
        "id": "qXGvIdpuYIIW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **CNN → Convolutional Neural Networks**"
      ],
      "metadata": {
        "id": "qH4jxJQ4T6Ay"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Start with **input image (or data)** – break it into small patches.\n",
        "\n",
        "- Apply **filters (kernels)** – slide over input – perform element-wise multiplication – calculate features (edges, textures, etc.).\n",
        "\n",
        "     **`Feature Map = ∑ (Filter × Image Patch) + Bias`**\n",
        "\n",
        "- Apply activation function (usually ReLU) – keep only important features.\n",
        "\n",
        "    **`𝑦 = Activation ( ∑ (Filter × Patch) + 𝑏 )`**\n",
        "\n",
        "- Apply **Pooling (e.g., Max Pooling)** – reduces size – keeps strongest features.\n",
        "\n",
        "    **`Pooling = Pick maximum or average value from patch`**\n",
        "\n",
        "- Repeat: Convolution → Activation → Pooling to learn deeper patterns.\n",
        "\n",
        "  -Flatten the final feature map – turn it into a vector.\n",
        "\n",
        "- Pass to **Fully Connected Layers (like ANN)** to make predictions.\n",
        "\n",
        "- Calculate Loss: Compare predicted output with actual value.\n",
        "\n",
        "- Improve Accuracy using **Backpropagation**: Send error backward – update filter and dense layer weights.\n",
        "\n",
        "    **`𝑤 = 𝑤 − 𝛼 ⋅ ∂ Loss / ∂ 𝑤`**\n",
        "\n",
        "- Repeat Until Convergence – train for multiple epochs until model predicts correctly."
      ],
      "metadata": {
        "id": "1-liSZCaS3Dq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **RNN → Recurrent Neural Networks**"
      ],
      "metadata": {
        "id": "X2I2pSIsV1Y6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Start with **input data (like sequence/text/time series)** – pass one value at a time (step-by-step).\n",
        "\n",
        "- Each step: Multiply input by weight, add previous hidden state (memory), apply activation.\n",
        "\n",
        "    **`ℎₜ = Activation ( 𝑤ₓ ⋅ 𝑥ₜ + 𝑤ₕ ⋅ ℎₜ₋₁ + 𝑏 )`**\n",
        "\n",
        "- Pass the new hidden state ℎₜ to the next step – this helps the network \"remember\" past info.\n",
        "\n",
        "- Final output is compared with actual value – calculate loss.\n",
        "\n",
        "- Backpropagation Through Time (BPTT): Send error backward through all time steps – update weights.\n",
        "\n",
        "    **`𝑤 = 𝑤 − 𝛼 ⋅ ∂ Loss / ∂ 𝑤`**\n",
        "\n",
        "- Repeat until convergence – train over sequences multiple times.\n"
      ],
      "metadata": {
        "id": "QV6jC11ATZ8q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**LSTM (Long Short-Term Memory)**\n",
        "\n",
        "Same as RNN but with **extra gates** to **better handle long-term memory**.\n",
        "\n",
        "- **Forget Gate**: Decides what to throw away from memory.\n",
        "- **Input Gate**: Decides what new info to add.\n",
        "- **Output Gate**: Decides what to show as output.\n",
        "\n",
        "All gates work together to update the cell state (memory) and hidden state.\n",
        "\n",
        "This solves the problem of **vanishing gradient** in long sequences.\n",
        "\n",
        "|\n",
        "\n",
        "**GRU (Gated Recurrent Unit)**\n",
        "\n",
        "Similar to LSTM but **simpler** – uses only **2 gates**:\n",
        "\n",
        "- **Update Gate**: Controls both memory and input.\n",
        "- **Reset Gate**: Controls how much past info to forget.\n",
        "\n",
        "Faster and lighter than LSTM, often with similar performance."
      ],
      "metadata": {
        "id": "TmnfAboFWX3I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Transformer**"
      ],
      "metadata": {
        "id": "iCYx4FTHFKi7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Input Preparation**\n",
        "- Take **input sequence** (e.g., sentence), break it into tokens (words) and convert to **embedding vectors**. Add **Positional Encoding** to add word order\n",
        "- These ordered vector will passed into the first **encoder layer**\n",
        "\n",
        "**Encoder Layer**\n",
        "- Each **encoder layer** has -\n",
        "  - **Multi-Head Self-Attention (with Add & Normalize)**\n",
        "    - **self-attention** → each word looks at all other words including itself to determine their importance.\n",
        "    - Output of self-attention is added to the original input (Add), and then cleaned up (Normalize).\n",
        "  - **Feedforward Layer (with Add & Normalize)**\n",
        "    - improves the word’s vector even more. Again, it adds the original input and normalizes the result.\n",
        "- The **Output** is passed to the **next encoder layer**\n",
        "- After all encoder layers are done, we get a **new vector** for each word. These vectors now understand each word’s **meaning** in context.\n",
        "\n",
        "**Decoder Layer**\n",
        "- The **decoder** takes the final output from the encoder (which understands the input sentence).\n",
        "- Each **Decoder Layer** has -\n",
        "  - **Masked Self-Attention**\n",
        "    - each word can only see the previous words, not future ones (prevents cheating)\n",
        "  - **Encoder-Decoder Attention (with Add & Normalize)**\n",
        "    - decoder looks at the encoder output to understand the input sentence.\n",
        "  - **Feedforward Layer (with Add & Normalize)**\n",
        "- After all decoder layers, we send the **output** to a **linear layer + softmax**\n",
        "- Use **Loss Function** to compare prediction with actual\n",
        "- Use **Backpropagation** to update weights and learn\n",
        "- Repeat over and over (many epochs) until it learns well."
      ],
      "metadata": {
        "id": "bGIPrftLJAcK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**input preparation**\n",
        "\n",
        "- Tokenization → ['The', 'dog', 'barked']\n",
        "- Embedding → Each word becomes a vector (e.g., n/512-dimensional)\n",
        "- Positional Encoding → 0, 1, 2 and so on\n",
        "\n",
        "**encoder**\n",
        "\n",
        "- a. Self-Attention → Each word checks the importance of all other words, including itself.\n",
        "  - Each word is turned into three vectors: Query (Q), Key (K), Value (V)\n",
        "  - Attention(Q, K, V) = softmax(QKᵀ / √dk) V\n",
        "- Now each word gets a new, context-aware vector.\n",
        "-  b. Feedforward Layer → Each word vector passes through a small neural network, This helps the model learn abstract features like tense, roles, etc.\n",
        "  - FFN(x) = max(0, xW₁ + b₁)W₂ + b₂\n",
        "- Add & Normalize after both a and b steps.\n",
        "\n",
        "**decoder**\n",
        "\n",
        "Assume we want to translate:\n",
        "\"The dog barked\" → \"Le chien a aboyé\" (French)\n",
        "\n",
        "-  a. Masked Self-Attention → The decoder only sees past part of the output. Masking prevents the model from peeking\n",
        "  - If generating \"Le\", it sees nothing (start token only)\n",
        "  - \"chien\", it sees [\"Le\"]\n",
        "  - \"a\", it sees [\"Le\", \"chien\"]\n",
        "- b. Encoder-Decoder Attention → While generating each word, the decoder looks at relevant parts of the input.\n",
        "  - When generating “aboyé”, it focuses on \"barked\"\n",
        "- c. Feedforward → refines the output vectors.\n",
        "\n",
        "**output**\n",
        "\n",
        "- Each decoder step outputs a word, one by one:\n",
        "  - \"Le\" → \"chien\" → \"a\" → \"aboyé\"\n",
        "\n",
        "- These come from a Linear layer + Softmax, choosing the most likely word.\n",
        "\n",
        "\n",
        "**training**\n",
        "- Compare generated output (\"Le chien a aboyé\") with the actual correct output.\n",
        "\n",
        "- Use loss function (e.g., Cross-Entropy Loss).\n",
        "\n",
        "- Apply Backpropagation to adjust model weights.\n",
        "\n",
        "- Repeat across many sentences and epochs until the model gets better."
      ],
      "metadata": {
        "id": "pGqfkMEMrp19"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**BERT (Bidirectional Encoder Representations from Transformers)**\n",
        "\n",
        "- Uses only the **Encoder** part of Transformer\n",
        "- Reads sentence **in both directions (left + right)** for full context\n",
        "- Pretrained using **Masked Language Modeling** (hide random words and guess them)\n",
        "- Good for **understanding tasks**: classification, question answering, etc.\n",
        "\n",
        "|\n",
        "\n",
        "**GPT (Generative Pretrained Transformer)**\n",
        "\n",
        "- Uses only the **Decoder** part of Transformer\n",
        "- Reads text **left to right (one direction only)**\n",
        "- Pretrained to **predict the next word** in sequence\n",
        "- Great for **generating text**: writing, answering, summarizing, etc."
      ],
      "metadata": {
        "id": "uW59G7BjKuMT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Autoencoder**"
      ],
      "metadata": {
        "id": "9hyjg4FmLcw7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Input data (e.g., image or vector) is passed to the model.\n",
        "- **Encoder** compresses the input into a smaller representation (**latent vector**).\n",
        "- **Decoder** tries to **reconstruct the original input** from the latent vector.\n",
        "- Output is compared with original input → calculate **Reconstruction Loss**.\n",
        "- Backpropagation is used to adjust weights and **minimize reconstruction error**.\n",
        "- Trained to **learn only the essential features** of input data."
      ],
      "metadata": {
        "id": "Ts0gslv1Nxl6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Variational Autoencoder (VAE)**\n",
        "\n",
        "- Works like autoencoder but **adds randomness** to latent space.\n",
        "- Encoder doesn't give just one latent vector → it gives **mean (μ)** and **variance (σ)**.\n",
        "- From μ and σ → sample a **random point (z)** in latent space.\n",
        "- This makes the model **generate new, similar data** (good for generative tasks).\n",
        "- Loss = Reconstruction Loss + KL Divergence (regularizes latent space)\n",
        "- Trained to **generate new data** and learn **smooth, continuous features**."
      ],
      "metadata": {
        "id": "XSRi-RHaOBza"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Generative Adversarial Network (GAN)**"
      ],
      "metadata": {
        "id": "CBAVHtNFQQRd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- GAN has **two models**:  \n",
        "  - **Generator**: creates fake data  \n",
        "  - **Discriminator**: checks if data is real or fake  \n",
        "- Start with random noise as input → **Generator** tries to create realistic data (e.g., fake images).\n",
        "- **Discriminator** sees both real and fake data → predicts if it's real (1) or fake (0).\n",
        "- Both models compete:\n",
        "  - Generator improves to **fool the discriminator**\n",
        "  - Discriminator improves to **catch fakes**\n",
        "- Loss:\n",
        "  - Generator tries to **maximize** the chance of fooling the discriminator\n",
        "  - Discriminator tries to **minimize** its error in detecting fake vs real\n",
        "- Use **backpropagation** to update both models\n",
        "- Trained until **Generator produces realistic data** that Discriminator can’t easily detect"
      ],
      "metadata": {
        "id": "WopFws3WPZXS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Deep Reinforcement Learning (DRL)**"
      ],
      "metadata": {
        "id": "gB_VAVBkS7Ur"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- An **agent** interacts with an **environment** (like a game or simulation).\n",
        "- At each step:\n",
        "  - Agent observes the **state** of the environment.\n",
        "  - Chooses an **action** based on current state.\n",
        "  - Environment returns a **reward** and new **state**.\n",
        "- Goal: **Learn actions** that maximize total reward over time.\n",
        "- Uses **Deep Neural Networks** to approximate the best actions (called a policy or Q-function).\n",
        "- Learns by **trial and error**: good actions get **positive rewards**, bad ones get **penalties**.\n",
        "- Uses **backpropagation** to update the neural network and improve decisions.\n",
        "- Trained over many episodes until the agent learns the best strategy."
      ],
      "metadata": {
        "id": "UnZdXdJZS0Rb"
      }
    }
  ]
}